{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Contextual Bandits\n",
    "\n",
    "This tutorial contains a simple example of how to build a policy-gradient based agent that can solve the contextual bandit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Contextual Bandits\n",
    "\n",
    "Here we define our contextual bandits. In this example, we are using three four-armed bandits. What this means is that each bandit has four arms that can be pulled. Each bandit has different success probabilities for each arm, and as such, requires different actions to obtain the best result. The `pull_bandit` function generates a random number from a normal distribution with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit-arm that will most often give a positive reward, depending on the bandit presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualBandit(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        \n",
    "        # Currently arms 4, 2, and 1 (respectively) are the most optimal\n",
    "        self.bandits = np.array([[.2, 0., -0., -5.], [.1, -5., 1., .25], [-5., 5., 5., 5.]])\n",
    "        self.n_bandits = self.bandits.shape[0]\n",
    "        self.n_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def get_bandit(self):\n",
    "        \"\"\" returns a random state for each episode \"\"\"\n",
    "        self.state = np.random.randint(0, len(self.bandits))\n",
    "        return self.state\n",
    "    \n",
    "    def pull_arm(self, action):\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        return 1 if result > bandit else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Policy-Based Agent\n",
    "\n",
    "The code below establishes our simple neural agent. It takes as input the current state, and returns an action. This allows the agent to take actions which are conditioned on the state of the environment - a critical step toward being able to solve full RL problems.\n",
    "\n",
    "The agent uses a single set of weights, within which each value is an estimate of the value of the return from choosing a particular arm given a bandit. We use a policy gradient method to update the agent by moving the value for the selected action towards the received reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, learning_rate, state_dim, n_actions):\n",
    "        # These lines established the feed-forward part of the network.\n",
    "        # The agent takes a state and produces an action.\n",
    "        self.state_in = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        state_in_one_hot = slim.one_hot_encoding(self.state_in, state_dim)\n",
    "        output = slim.fully_connected(state_in_one_hot, n_actions,\n",
    "                                      activation_fn=tf.nn.sigmoid,\n",
    "                                      weights_initializer=tf.ones_initializer(),\n",
    "                                      biases_initializer=None)\n",
    "        self.output = tf.reshape(output, [-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        self.reward_ph = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "        self.action_ph = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "        self.responsible_weight = tf.slice(self.output, self.action_ph, [1])\n",
    "        self.loss = -tf.log(self.responsible_weight) * self.reward_ph\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        self.update_op = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "\n",
    "We train our agent by getting a state from the environment, taking an action, and recieving a reward. Using these three things, we know how to update our network in order to more often choose actions given states that will yield the highest rewards over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "cbandit = ContextualBandit()\n",
    "agent = Agent(learning_rate=0.001, state_dim=cbandit.n_bandits, n_actions=cbandit.n_actions)\n",
    "W = tf.trainable_variables()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams:\n",
    "n_episodes = 10000\n",
    "total_reward = np.zeros([cbandit.n_bandits, cbandit.n_actions])  # set scoreboard for bandits to zeros\n",
    "epsilon = 0.1  # set the chance of taking a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 3 bandits: [ 0.    0.   -0.25]\n",
      "Mean reward for each of the 3 bandits: [34.5  42.   34.25]\n",
      "Mean reward for each of the 3 bandits: [72.   81.25 69.5 ]\n",
      "Mean reward for each of the 3 bandits: [110.75 117.   106.5 ]\n",
      "Mean reward for each of the 3 bandits: [152.5  152.   143.25]\n",
      "Mean reward for each of the 3 bandits: [192.25 188.75 177.75]\n",
      "Mean reward for each of the 3 bandits: [227.5  229.25 213.  ]\n",
      "Mean reward for each of the 3 bandits: [264.5  267.5  249.25]\n",
      "Mean reward for each of the 3 bandits: [301.5  300.25 289.  ]\n",
      "Mean reward for each of the 3 bandits: [334.5  342.5  321.75]\n",
      "Mean reward for each of the 3 bandits: [371.   383.   355.25]\n",
      "Mean reward for each of the 3 bandits: [407.75 422.25 390.75]\n",
      "Mean reward for each of the 3 bandits: [451.   457.5  419.75]\n",
      "Mean reward for each of the 3 bandits: [486.   499.5  452.75]\n",
      "Mean reward for each of the 3 bandits: [523.5  539.75 486.  ]\n",
      "Mean reward for each of the 3 bandits: [556.5  578.75 523.5 ]\n",
      "Mean reward for each of the 3 bandits: [592.75 616.75 557.75]\n",
      "Mean reward for each of the 3 bandits: [634.   648.25 591.5 ]\n",
      "Mean reward for each of the 3 bandits: [675.   686.25 625.  ]\n",
      "Mean reward for each of the 3 bandits: [714.5  725.5  659.75]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(n_episodes):\n",
    "        s = cbandit.get_bandit()  # get a state from the environment\n",
    "        \n",
    "        # explore else exploit\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = np.random.randint(cbandit.n_actions)\n",
    "        else:\n",
    "            action = sess.run(agent.chosen_action, feed_dict={agent.state_in: [s]})\n",
    "            \n",
    "        reward = cbandit.pull_arm(action)  # get our reward for taking an action given a bandit\n",
    "        \n",
    "        # Update the network\n",
    "        feed_dict = {agent.reward_ph: [reward], agent.action_ph: [action], agent.state_in: [s]}\n",
    "        _, W1 = sess.run([agent.update_op, W], feed_dict)\n",
    "        \n",
    "        # Update the running tally of rewards\n",
    "        total_reward[s, action] += reward\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print('Mean reward for each of the %i bandits: %s' % \n",
    "                  (cbandit.n_bandits, str(np.mean(total_reward, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99623156, 1.00242   , 0.9975794 , 1.6312536 ],\n",
       "       [0.9997333 , 1.6507342 , 0.98159117, 0.99757993],\n",
       "       [1.635012  , 0.97640234, 0.9706452 , 0.9769494 ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent thinks action 4 for bandit 1 is the most promising\n",
      "and it is right!\n",
      "\n",
      "The agent thinks action 2 for bandit 2 is the most promising\n",
      "and it is right!\n",
      "\n",
      "The agent thinks action 1 for bandit 3 is the most promising\n",
      "and it is right!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(cbandit.n_bandits):\n",
    "    print('The agent thinks action %s for bandit %i is the most promising' %\n",
    "          (str(np.argmax(W1[i]) + 1), i + 1))\n",
    "    \n",
    "    if np.argmax(W1[i]) == np.argmin(cbandit.bandits[i]):\n",
    "        print('and it is right!')\n",
    "    else:\n",
    "        print('and it is wrong!')\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
