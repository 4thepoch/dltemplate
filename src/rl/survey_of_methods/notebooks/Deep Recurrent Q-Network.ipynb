{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Recurrent Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a Deep Recurrent Q-Network, which can solve Partially Observable Markov Decision Processes.\n",
    "\n",
    "To learn more about DRQNs, see my blog post on them here: https://medium.com/p/68463e9aeefc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from gridworld import GameEnvironment\n",
    "import gym\n",
    "from helper import process_state, save_to_monitor, update_target, update_target_graph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tqdm import trange\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Game Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to adjust the size of the gridworld. Making it smaller (adjusting size) provides an easier task for our DRQN agent, while making the world larger increases the challenge.\n",
    "\n",
    "Initializing the Gridworld with True limits the field of view, resulting in a partially observable MDP. Initializing it with False provides the agent with the entire environment, resulting in a fully observable MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GameEnvironment(partial=True, size=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of a starting environment in our simple game. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to the green square (for +1 reward) and avoid the red square (for -1 reward). When the agent moves through a green or red square, it is randomly moved to a new place in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(object):\n",
    "    \n",
    "    def __init__(self, n_hidden, rnn_cell, scope):\n",
    "        # The network receives a frame from the game, flattened into an array.\n",
    "        # It then resizes and processes it through four convolutional layers.\n",
    "        self.scalar_input = tf.placeholder(shape=[None, 21168], dtype=tf.float32)\n",
    "        self.image_in = tf.reshape(self.scalar_input, shape=[-1, 84, 84, 3])\n",
    "        self.conv1 = slim.conv2d(inputs=self.image_in, num_outputs=32,\n",
    "                                 kernel_size=[8, 8], stride=[4, 4], padding='VALID', \n",
    "                                 biases_initializer=None, scope=scope + '_conv1')\n",
    "        self.conv2 = slim.conv2d(inputs=self.conv1, num_outputs=64,\n",
    "                                 kernel_size=[4, 4], stride=[2, 2], padding='VALID', \n",
    "                                 biases_initializer=None, scope=scope + '_conv2')\n",
    "        self.conv3 = slim.conv2d(inputs=self.conv2, num_outputs=64,\n",
    "                                 kernel_size=[3, 3], stride=[1, 1], padding='VALID', \n",
    "                                 biases_initializer=None, scope=scope + '_conv3')\n",
    "        self.conv4 = slim.conv2d(inputs=self.conv3, num_outputs=n_hidden,\n",
    "                                 kernel_size=[7, 7], stride=[1, 1], padding='VALID', \n",
    "                                 biases_initializer=None, scope=scope + '_conv4')\n",
    "        \n",
    "        self.sequence_length = tf.placeholder(dtype=tf.int32)\n",
    "        \n",
    "        # Take the output from the final convolutional layer and send it \n",
    "        # to a recurrent layer. The input must be reshaped into \n",
    "        # [batch, trace, units] for RNN processing, and then returned to\n",
    "        # [batch, units] when sent through the upper levels.\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "        self.conv_flat = tf.reshape(slim.flatten(self.conv4), [self.batch_size, self.sequence_length, n_hidden])\n",
    "        self.state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn, self.rnn_state = tf.nn.dynamic_rnn(inputs=self.conv_flat, cell=rnn_cell, dtype=tf.float32, \n",
    "                                                     initial_state=self.state_in, scope=scope + '_rnn')\n",
    "        self.rnn = tf.reshape(self.rnn, shape=[-1, n_hidden])\n",
    "        \n",
    "        # The output from the recurrent layer is then split into separate \n",
    "        # Advantage and Value streams.\n",
    "        self.stream_a, self.stream_v = tf.split(self.rnn, 2, 1)\n",
    "        self.aw = tf.Variable(tf.random_normal([n_hidden // 2, 4]))\n",
    "        self.vw = tf.Variable(tf.random_normal([n_hidden // 2, 1]))\n",
    "        self.advantage = tf.matmul(self.stream_a, self.aw)\n",
    "        self.value = tf.matmul(self.stream_v, self.vw)\n",
    "        self.salience = tf.gradients(self.advantage, self.image_in)\n",
    "        \n",
    "        # then combine them together to get our final Q-values\n",
    "        self.q_out = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.predict = tf.argmax(self.q_out, 1)\n",
    "        \n",
    "        # obtain the loss by taking the sum of squares difference between \n",
    "        # the target and predicted Q-values\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, 4, dtype=tf.float32)\n",
    "        \n",
    "        self.q = tf.reduce_sum(tf.multiply(self.q_out, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.target_q - self.q)\n",
    "        \n",
    "        # In order to only propagate accurate gradients through the network,\n",
    "        # we will mask the first half of the losses for each trace as per\n",
    "        # Lample & Chatlot 2016\n",
    "        self.mask_a = tf.zeros([self.batch_size, self.sequence_length // 2])\n",
    "        self.mask_b = tf.ones([self.batch_size, self.sequence_length // 2])\n",
    "        self.mask = tf.concat([self.mask_a, self.mask_b], 1)\n",
    "        self.mask = tf.reshape(self.mask, [-1])\n",
    "        self.loss = tf.reduce_mean(self.td_error * self.mask)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.update_op = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "This class lets us store experiences, then sample them randomly to train the network. Episode buffer stores experiences for each individal episode. Experience buffer stores entire episodes of experience, and sample() allows us to get training batches needed from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer(object):\n",
    "    \n",
    "    def __init__(self, buffer_size=1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        n = len(self.buffer) + 1\n",
    "        if n >= self.buffer_size:\n",
    "            self.buffer[0:n-self.buffer_size] = []\n",
    "            \n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size, trace_length):\n",
    "        sampled_episodes = random.sample(self.buffer, batch_size)\n",
    "        sampled_traces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0, len(episode) + 1 - trace_length)\n",
    "            sampled_traces.append(episode[point:point+trace_length])\n",
    "            \n",
    "        return np.reshape(np.array(sampled_traces), [batch_size * trace_length, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams:\n",
    "\n",
    "batch_size = 4  # number experience traces to use for each training step\n",
    "trace_length = 8  # length of each experience trace when training\n",
    "update_freq = 5  # frequency of performing a training step\n",
    "gamma = 0.99  # discount factor on the target Q-values\n",
    "start_epsilon = 1  # starting chance of random action\n",
    "end_epsilon = 0.1  # final chance of random action\n",
    "annealing_steps = 10000  # number steps of training to reduce start_epsilon to end_epsilon\n",
    "#n_episodes = 10000  # number episodes of game environment to train network\n",
    "n_episodes = 500\n",
    "n_pretrain_steps = 10000  # number steps of random actions before training begins\n",
    "max_episode_length = 50  # max allowed length of our episode\n",
    "n_hidden = 512  # size of final convolutional layer before splitting it into Advantage and Value streams\n",
    "time_per_step = 1  # length of each step used in gif creation\n",
    "summary_length = 100  # number episodes to periodically save for analysis\n",
    "tau = 0.001\n",
    "load_model = False  # whether to load a saved model\n",
    "save_path = 'drqn'  # path to save our model to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning should occur in a couple hours on a moderately powerful machine (GTX970). (Getting Atari games to work will take at least a day of training on a powerful machine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # we define the cells for the primary and target Q-networks\n",
    "    cell_main = tf.contrib.rnn.BasicLSTMCell(num_units=n_hidden, state_is_tuple=True)\n",
    "    cell_target = tf.contrib.rnn.BasicLSTMCell(num_units=n_hidden, state_is_tuple=True)\n",
    "    main_network = QNetwork(n_hidden, cell_main, 'main')\n",
    "    target_network = QNetwork(n_hidden, cell_target, 'target')\n",
    "    saver = tf.train.Saver()\n",
    "    trainables = tf.trainable_variables()\n",
    "    target_ops = update_target_graph(trainables, tau)\n",
    "    buffer = ExperienceBuffer()\n",
    "    epsilon = start_epsilon\n",
    "    step_drop = (start_epsilon - end_epsilon) / annealing_steps\n",
    "    js, rewards = [], []\n",
    "    n_steps = 0\n",
    "    \n",
    "    # make path to save model, unless path already exists\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    # write the first line of the master log-file for the Control Center\n",
    "    with open('monitor/log.csv', 'w') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        writer.writerow(['Episode', 'Length', 'Reward', 'IMG', 'LOG', 'SAL'])\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        if load_model:\n",
    "            print('Loading model...')\n",
    "            ckpt = tf.train.get_checkpoint_state(save_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(n_episodes):\n",
    "            episode_buffer = []\n",
    "            \n",
    "            # reset environment and get first new observation\n",
    "            s_p = env.reset()\n",
    "            s = process_state(s_p)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            j = 0\n",
    "            \n",
    "            # reset the recurrent layer's hidden state\n",
    "            state = (np.zeros([1, n_hidden]), np.zeros([1, n_hidden]))\n",
    "            \n",
    "            # Train the Q-network\n",
    "            # if the agent takes longer to reach either of the blocks, then end the trial\n",
    "            while j < max_episode_length:\n",
    "                j += 1\n",
    "                \n",
    "                # choose an action greedily from the Q-network, \n",
    "                # with epsilon chance of random action\n",
    "                if np.random.rand(1) < epsilon or n_steps < n_pretrain_steps:\n",
    "                    state1 = sess.run(main_network.rnn_state, feed_dict={\n",
    "                        main_network.scalar_input: [s / 255.],\n",
    "                        main_network.sequence_length: 1,\n",
    "                        main_network.state_in: state,\n",
    "                        main_network.batch_size: 1\n",
    "                    })\n",
    "                    a = np.random.randint(0, 4)\n",
    "                else:\n",
    "                    a, state1 = sess.run([main_network.predict, main_network.rnn_state], feed_dict={\n",
    "                        main_network.scalar_input: [s / 255.],\n",
    "                        main_network.sequence_length: 1,\n",
    "                        main_network.state_in: state,\n",
    "                        main_network.batch_size: 1\n",
    "                    })\n",
    "                    a = a[0]\n",
    "                \n",
    "                s1_p, reward, done = env.step(a)\n",
    "                s1 = process_state(s1_p)\n",
    "                n_steps += 1\n",
    "                \n",
    "                # save the experience to our episode buffer\n",
    "                episode_buffer.append(np.reshape(np.array([s, a, reward, s1, done]), [1, 5]))\n",
    "                \n",
    "                if n_steps > n_pretrain_steps:\n",
    "                    if epsilon > end_epsilon:\n",
    "                        epsilon -= step_drop\n",
    "                        \n",
    "                    if n_steps % update_freq == 0:\n",
    "                        update_target(target_ops, sess)\n",
    "                        state_train = (np.zeros([batch_size, n_hidden]), np.zeros([batch_size, n_hidden]))\n",
    "                        \n",
    "                        # get a random batch of experiences\n",
    "                        train_batch = buffer.sample(batch_size, trace_length)\n",
    "                        \n",
    "                        # perform the Double-DQN update to the target Q-values\n",
    "                        q1 = sess.run(main_network.predict, feed_dict={\n",
    "                            main_network.scalar_input: np.vstack(train_batch[:, 3] / 255.),\n",
    "                            main_network.sequence_length: trace_length,\n",
    "                            main_network.state_in: state_train,\n",
    "                            main_network.batch_size: batch_size\n",
    "                        })\n",
    "                        q2 = sess.run(target_network.q_out, feed_dict={\n",
    "                            target_network.scalar_input: np.vstack(train_batch[:, 3] / 255.),\n",
    "                            target_network.sequence_length: trace_length,\n",
    "                            target_network.state_in: state_train,\n",
    "                            target_network.batch_size: batch_size\n",
    "                        })\n",
    "                        end_multiplier = -train_batch[:, 4] - 1\n",
    "                        double_q = q2[range(batch_size * trace_length), q1]\n",
    "                        target_q = train_batch[:, 2] + gamma * double_q * end_multiplier\n",
    "                        \n",
    "                        # update the network with our target values\n",
    "                        _ = sess.run(main_network.update_op, feed_dict={\n",
    "                            main_network.scalar_input: np.vstack(train_batch[:, 0] / 255.),\n",
    "                            main_network.target_q: target_q,\n",
    "                            main_network.actions: train_batch[:, 1],\n",
    "                            main_network.sequence_length: trace_length,\n",
    "                            main_network.state_in: state_train,\n",
    "                            main_network.batch_size: batch_size\n",
    "                        })\n",
    "                        \n",
    "                total_reward += reward\n",
    "                s = s1\n",
    "                s_p = s1_p\n",
    "                state = state1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            buffer_array = np.array(episode_buffer)\n",
    "            episode_buffer = list(zip(buffer_array))\n",
    "            buffer.add(episode_buffer)\n",
    "            js.append(j)\n",
    "            rewards.append(total_reward)\n",
    "            \n",
    "            # periodically save the model\n",
    "            if i % 1000 == 0:\n",
    "                saver.save(sess, '{}/model-{}'.format(save_path, i))\n",
    "                print('Saved model')\n",
    "                \n",
    "            if len(rewards) % 100 == 0 and i != 0:\n",
    "                print('Number steps:', n_steps, 'mean reward:', np.mean(rewards[-10:]), 'epsilon:', epsilon)\n",
    "\n",
    "                # record performance metrics and episode logs for the Control Center\n",
    "                save_to_monitor(i, rewards, js, np.reshape(np.array(episode_buffer), [len(episode_buffer), 5]), \n",
    "                                summary_length, n_hidden, sess, main_network, time_per_step)\n",
    "                \n",
    "        saver.save(sess, '{}/model-{}'.format(save_path, i))\n",
    "        \n",
    "    print('Successful episodes %:', str(sum(rewards) / n_episodes))\n",
    "    \n",
    "    return rewards, js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n",
      "Number steps: 5000 mean reward: 0.5 epsilon: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:00<00:00, 463.36it/s]\n",
      " 98%|█████████▊| 51/52 [00:00<00:00, 1410.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number steps: 10000 mean reward: 0.5 epsilon: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:00<00:00, 609.61it/s]\n",
      " 98%|█████████▊| 51/52 [00:00<00:00, 1516.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number steps: 15000 mean reward: 2.2 epsilon: 0.5499999999998275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:00<00:00, 628.65it/s]\n",
      " 98%|█████████▊| 51/52 [00:00<00:00, 1095.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number steps: 20000 mean reward: 1.1 epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:00<00:00, 651.87it/s]\n",
      " 98%|█████████▊| 51/52 [00:00<00:00, 1090.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number steps: 25000 mean reward: 0.6 epsilon: 0.09999999999985551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:00<00:00, 530.93it/s]\n",
      " 98%|█████████▊| 51/52 [00:00<00:00, 1588.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful episodes %: 0.804\n"
     ]
    }
   ],
   "source": [
    "rewards, _ = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Network Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faef958b160>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mean reward over time\n",
    "reward_mat = np.resize(np.array(rewards), [len(rewards) // 100, 100])\n",
    "mean_reward = np.average(reward_mat, 1)\n",
    "plt.plot(mean_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams:\n",
    "\n",
    "epsilon = 0.1  # chance of random action\n",
    "#n_episodes = 10000  # number episodes of game environment to train network\n",
    "n_episodes = 500\n",
    "max_episode_length = 50  # max allowed length of our episode\n",
    "n_hidden = 512  # size of final convolutional layer before splitting it into Advantage and Value streams\n",
    "time_per_step = 1  # length of each step used in gif creation\n",
    "summary_length = 100  # number episodes to periodically save for analysis\n",
    "load_model = True  # whether to load a saved model\n",
    "save_path = 'drqn'  # path to save our model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # we define the cells for the primary and target Q-networks\n",
    "    cell_main = tf.contrib.rnn.BasicLSTMCell(num_units=n_hidden, state_is_tuple=True)\n",
    "    cell_target = tf.contrib.rnn.BasicLSTMCell(num_units=n_hidden, state_is_tuple=True)\n",
    "    main_network = QNetwork(n_hidden, cell_main, 'main')\n",
    "    target_network = QNetwork(n_hidden, cell_target, 'target')\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    js, rewards = [], []\n",
    "    n_steps = 0\n",
    "    \n",
    "    # make path to save model, unless path already exists\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    # write the first line of the master log-file for the Control Center\n",
    "    with open('monitor/log.csv', 'w') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        writer.writerow(['Episode', 'Length', 'Reward', 'IMG', 'LOG', 'SAL'])\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        if load_model:\n",
    "            print('Loading model...')\n",
    "            ckpt = tf.train.get_checkpoint_state(save_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(n_episodes):\n",
    "            episode_buffer = []\n",
    "            \n",
    "            # reset environment and get first new observation\n",
    "            s_p = env.reset()\n",
    "            s = process_state(s_p)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            j = 0\n",
    "            \n",
    "            # reset the recurrent layer's hidden state\n",
    "            state = (np.zeros([1, n_hidden]), np.zeros([1, n_hidden]))\n",
    "            \n",
    "            # Train the Q-network\n",
    "            # if the agent takes longer to reach either of the blocks, then end the trial\n",
    "            while j < max_episode_length:\n",
    "                j += 1\n",
    "                \n",
    "                # choose an action greedily from the Q-network, \n",
    "                # with epsilon chance of random action\n",
    "                if np.random.rand(1) < epsilon:\n",
    "                    state1 = sess.run(main_network.rnn_state, feed_dict={\n",
    "                        main_network.scalar_input: [s / 255.],\n",
    "                        main_network.sequence_length: 1,\n",
    "                        main_network.state_in: state,\n",
    "                        main_network.batch_size: 1\n",
    "                    })\n",
    "                    a = np.random.randint(0, 4)\n",
    "                else:\n",
    "                    a, state1 = sess.run([main_network.predict, main_network.rnn_state], feed_dict={\n",
    "                        main_network.scalar_input: [s / 255.],\n",
    "                        main_network.sequence_length: 1,\n",
    "                        main_network.state_in: state,\n",
    "                        main_network.batch_size: 1\n",
    "                    })\n",
    "                    a = a[0]\n",
    "                \n",
    "                s1_p, reward, done = env.step(a)\n",
    "                s1 = process_state(s1_p)\n",
    "                n_steps += 1\n",
    "                \n",
    "                # save the experience to our episode buffer\n",
    "                episode_buffer.append(np.reshape(np.array([s, a, reward, s1, done]), [1, 5]))\n",
    "                \n",
    "                total_reward += reward\n",
    "                s = s1\n",
    "                s_p = s1_p\n",
    "                state = state1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            buffer_array = np.array(episode_buffer)\n",
    "            js.append(j)\n",
    "            rewards.append(total_reward)\n",
    "            \n",
    "            # periodically save the model\n",
    "            if len(rewards) % summary_length == 0 and i != 0:\n",
    "                print('Number steps:', n_steps, 'mean reward:', np.mean(rewards[-10:]), 'epsilon:', epsilon)\n",
    "\n",
    "                # record performance metrics and episode logs for the Control Center\n",
    "                save_to_monitor(i, rewards, js, np.reshape(np.array(episode_buffer), [len(episode_buffer), 5]), \n",
    "                                summary_length, n_hidden, sess, main_network, time_per_step)\n",
    "                \n",
    "    print('Successful episodes %:', str(sum(rewards) / n_episodes))\n",
    "    \n",
    "    return rewards, js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "INFO:tensorflow:Restoring parameters from drqn/model-499\n",
      "Number steps: 5000 mean reward: 0.6 epsilon: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:00<00:00, 669.94it/s]\n",
      " 98%|█████████▊| 51/52 [00:00<00:00, 1244.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number steps: 10000 mean reward: 0.7 epsilon: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:00<00:00, 661.05it/s]\n",
      " 98%|█████████▊| 51/52 [00:00<00:00, 1277.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number steps: 15000 mean reward: 0.5 epsilon: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:00<00:00, 580.44it/s]\n",
      " 98%|█████████▊| 51/52 [00:00<00:00, 1227.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number steps: 20000 mean reward: 0.7 epsilon: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:00<00:00, 575.80it/s]\n",
      " 98%|█████████▊| 51/52 [00:00<00:00, 1080.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number steps: 25000 mean reward: 0.4 epsilon: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:00<00:00, 426.45it/s]\n",
      " 98%|█████████▊| 51/52 [00:00<00:00, 1360.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful episodes %: 0.742\n"
     ]
    }
   ],
   "source": [
    "rewards, _ = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
