{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks and Beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a Deep Q-Network using both Double DQN and Dueling DQN.\n",
    "\n",
    "The agent learns to solve a navigation task in a basic grid world.\n",
    "\n",
    "This model makes a few improvements on the ordinary Q-network:\n",
    "\n",
    "1. Going from a single-layer network to a multi-layer convolutional network.\n",
    "2. Implementing Experience Replay, which allows our network to train itself using stored memories from it’s experience.\n",
    "3. Utilizing a second “target” network, which we use to compute target Q-values during updates.\n",
    "\n",
    "It was these three innovations that enabled the Google DeepMind team to achieve superhuman performance on dozens of Atari games using a DQN agent.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "Convolutional layers. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network. In this way, they act similarly to human receptive fields.\n",
    "\n",
    "Experience Replay. The basic idea is that by storing an agent’s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. Each of these experiences are stored as a tuple of <state,action,reward,next state>. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them.\n",
    "\n",
    "Separate Target Network. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner.\n",
    "\n",
    "Instead of updating the target network periodically and all at once, we will be updating it frequently, but slowly. This technique was introduced in another DeepMind paper (https://arxiv.org/pdf/1509.02971.pdf), where they found that it stabilized the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld import GameEnvironment\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tqdm import trange\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GameEnvironment(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of a starting environment in our simple game. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to the green square (for +1 reward) and avoid the red square (for -1 reward). The position of the three blocks is randomized every episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(object):\n",
    "    \n",
    "    def __init__(self, n_hidden, n_actions):\n",
    "        # The network receives a frame from the game, flattened into an array.\n",
    "        # It then resizes and processes it through four convolutional layers.\n",
    "        self.scalar_input = tf.placeholder(shape=[None, 21168], dtype=tf.float32)\n",
    "        self.image_in = tf.reshape(self.scalar_input, shape=[-1, 84, 84, 3])\n",
    "        self.conv1 = slim.conv2d(inputs=self.image_in, num_outputs=32,\n",
    "                                 kernel_size=[8, 8], stride=[4, 4], padding='VALID', \n",
    "                                 biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d(inputs=self.conv1, num_outputs=64,\n",
    "                                 kernel_size=[4, 4], stride=[2, 2], padding='VALID', \n",
    "                                 biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d(inputs=self.conv2, num_outputs=64,\n",
    "                                 kernel_size=[3, 3], stride=[1, 1], padding='VALID', \n",
    "                                 biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d(inputs=self.conv3, num_outputs=n_hidden,\n",
    "                                 kernel_size=[7, 7], stride=[1, 1], padding='VALID', \n",
    "                                 biases_initializer=None)\n",
    "        \n",
    "        # take the output from the final convolutional layer and split it into \n",
    "        # separate advantage and value streams\n",
    "        self.stream_ac, self.stream_vc = tf.split(self.conv4, 2, 3)\n",
    "        self.stream_a = slim.flatten(self.stream_ac)\n",
    "        self.stream_v = slim.flatten(self.stream_vc)\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.aw = tf.Variable(xavier_init([n_hidden // 2, n_actions]))\n",
    "        self.vw = tf.Variable(xavier_init([n_hidden // 2, 1]))\n",
    "        self.advantage = tf.matmul(self.stream_a, self.aw)\n",
    "        self.value = tf.matmul(self.stream_v, self.vw)\n",
    "        \n",
    "        # then combine them together to get our final Q-values\n",
    "        self.q_out = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.predict = tf.argmax(self.q_out, 1)\n",
    "        \n",
    "        # obtain the loss by taking the sum of squares difference between \n",
    "        # the target and predicted Q-values\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, n_actions, dtype=tf.float32)\n",
    "        \n",
    "        self.q = tf.reduce_sum(tf.multiply(self.q_out, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.target_q - self.q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.update_op = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "This class lets us store experiences, then sample them randomly to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer(object):\n",
    "    \n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        n = len(self.buffer) + len(experience)\n",
    "        if n >= self.buffer_size:\n",
    "            self.buffer[0:n-self.buffer_size] = []\n",
    "            \n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state(states):\n",
    "    \"\"\" resize game frames \"\"\"\n",
    "    return np.reshape(states, [21168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_graph(tf_vars, tau):\n",
    "    \"\"\" update the parameters of our target network with those of the primary network \"\"\"\n",
    "    n_vars = len(tf_vars)\n",
    "    op_holder = []\n",
    "    for i, var in enumerate(tf_vars[0:n_vars//2]):\n",
    "        op_holder.append(tf_vars[i + n_vars//2].assign(var.value() * tau + \n",
    "                                                       (1 - tau) * tf_vars[i + n_vars//2].value()))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams:\n",
    "\n",
    "batch_size = 32  # number experiences to use for each training step\n",
    "update_freq = 4  # frequency of performing a training step\n",
    "gamma = 0.99  # discount factor on the target Q-values\n",
    "start_epsilon = 1  # starting chance of random action\n",
    "end_epsilon = 0.1  # final chance of random action\n",
    "annealing_steps = 10000  # number steps of training to reduce start_epsilon to end_epsilon\n",
    "#n_episodes = 10000  # number episodes of game environment to train network\n",
    "n_episodes = 500\n",
    "n_pretrain_steps = 10000  # number steps of random actions before training begins\n",
    "max_episode_length = 50  # max allowed length of our episode\n",
    "n_hidden = 512  # size of final convolutional layer before splitting it into Advantage and Value streams\n",
    "tau = 0.001  # rate to update target network toward primary network\n",
    "load_model = False  # whether to load a saved model\n",
    "save_path = 'dqn'  # path to save our model to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning should occur in a couple hours on a moderately powerful machine (GTX970). (Getting Atari games to work will take at least a day of training on a powerful machine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    tf.reset_default_graph()\n",
    "    n_actions = env.actions\n",
    "    main_network = QNetwork(n_hidden, n_actions)\n",
    "    target_network = QNetwork(n_hidden, n_actions)\n",
    "    saver = tf.train.Saver()\n",
    "    trainables = tf.trainable_variables()\n",
    "    target_ops = update_target_graph(trainables, tau)\n",
    "    buffer = ExperienceBuffer()\n",
    "    epsilon = start_epsilon\n",
    "    step_drop = (start_epsilon - end_epsilon) / annealing_steps\n",
    "    js, rewards = [], []\n",
    "    n_steps = 0\n",
    "    \n",
    "    # make path to save model, unless path already exists\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if load_model:\n",
    "            print('Loading model...')\n",
    "            ckpt = tf.train.get_checkpoint_state(save_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "        for i in range(n_episodes):\n",
    "            episode_buffer = ExperienceBuffer()\n",
    "            \n",
    "            # reset environment and get first new observation\n",
    "            s = env.reset()\n",
    "            s = process_state(s)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            j = 0\n",
    "            \n",
    "            # Train the Q-network\n",
    "            # if the agent takes longer to reach either of the blocks, then end the trial\n",
    "            while j < max_episode_length:\n",
    "                j += 1\n",
    "                \n",
    "                # choose an action greedily from the Q-network, \n",
    "                # with epsilon chance of random action\n",
    "                if np.random.rand(1) < epsilon or n_steps < n_pretrain_steps:\n",
    "                    a = np.random.randint(0, 4)\n",
    "                else:\n",
    "                    a = sess.run(main_network.predict, feed_dict={main_network.scalar_input: [s]})[0]\n",
    "                \n",
    "                s1, reward, done = env.step(a)\n",
    "                s1 = process_state(s1)\n",
    "                n_steps += 1\n",
    "                \n",
    "                # save the experience to our episode buffer\n",
    "                episode_buffer.add(np.reshape(np.array([s, a, reward, s1, done]), [1, 5]))\n",
    "                \n",
    "                if n_steps > n_pretrain_steps:\n",
    "                    if epsilon > end_epsilon:\n",
    "                        epsilon -= step_drop\n",
    "                        \n",
    "                    if n_steps % update_freq == 0:\n",
    "                        # get a random batch of experiences\n",
    "                        train_batch = buffer.sample(batch_size)\n",
    "                        \n",
    "                        # perform the Double-DQN update to the target Q-values\n",
    "                        q1 = sess.run(main_network.predict, feed_dict={\n",
    "                            main_network.scalar_input: np.vstack(train_batch[:, 3])\n",
    "                        })\n",
    "                        q2 = sess.run(target_network.q_out, feed_dict={\n",
    "                            target_network.scalar_input: np.vstack(train_batch[:, 3])\n",
    "                        })\n",
    "                        end_multiplier = -train_batch[:, 4] - 1\n",
    "                        double_q = q2[range(batch_size), q1]\n",
    "                        target_q = train_batch[:, 2] + gamma * double_q * end_multiplier\n",
    "                        \n",
    "                        # update the network with our target values\n",
    "                        _ = sess.run(main_network.update_op, feed_dict={\n",
    "                            main_network.scalar_input: np.vstack(train_batch[:, 0]),\n",
    "                            main_network.target_q: target_q,\n",
    "                            main_network.actions: train_batch[:, 1]\n",
    "                        })\n",
    "                        # update the target network toward the primary network\n",
    "                        update_target(target_ops, sess)\n",
    "                        \n",
    "                total_reward += reward\n",
    "                s = s1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            buffer.add(episode_buffer.buffer)\n",
    "            js.append(j)\n",
    "            rewards.append(total_reward)\n",
    "            \n",
    "            # periodically save the model\n",
    "            if i % 1000 == 0:\n",
    "                saver.save(sess, '{}/model-{}'.format(save_path, i))\n",
    "                print('Saved model')\n",
    "                \n",
    "            if len(rewards) % 10 == 0:\n",
    "                print('Number steps:', n_steps, 'mean reward:', np.mean(rewards[-10:]), 'epsilon:', epsilon)\n",
    "                \n",
    "        saver.save(sess, '{}/model-{}'.format(save_path, i))\n",
    "        \n",
    "    print('Successful episodes %:', str(sum(rewards) / n_episodes))\n",
    "    \n",
    "    return rewards, js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n",
      "Number steps: 500 mean reward: 2.8 epsilon: 1\n",
      "Number steps: 1000 mean reward: 0.9 epsilon: 1\n",
      "Number steps: 1500 mean reward: 2.8 epsilon: 1\n",
      "Number steps: 2000 mean reward: 2.5 epsilon: 1\n",
      "Number steps: 2500 mean reward: 0.5 epsilon: 1\n",
      "Number steps: 3000 mean reward: 0.3 epsilon: 1\n",
      "Number steps: 3500 mean reward: 1.7 epsilon: 1\n",
      "Number steps: 4000 mean reward: 3.4 epsilon: 1\n",
      "Number steps: 4500 mean reward: 2.1 epsilon: 1\n",
      "Number steps: 5000 mean reward: 1.2 epsilon: 1\n",
      "Number steps: 5500 mean reward: 1.9 epsilon: 1\n",
      "Number steps: 6000 mean reward: 2.0 epsilon: 1\n",
      "Number steps: 6500 mean reward: 2.8 epsilon: 1\n",
      "Number steps: 7000 mean reward: 2.4 epsilon: 1\n",
      "Number steps: 7500 mean reward: 2.0 epsilon: 1\n",
      "Number steps: 8000 mean reward: 1.4 epsilon: 1\n",
      "Number steps: 8500 mean reward: 1.8 epsilon: 1\n",
      "Number steps: 9000 mean reward: 2.8 epsilon: 1\n",
      "Number steps: 9500 mean reward: 2.2 epsilon: 1\n",
      "Number steps: 10000 mean reward: 2.8 epsilon: 1\n",
      "Number steps: 10500 mean reward: 1.8 epsilon: 0.9549999999999828\n",
      "Number steps: 11000 mean reward: 3.8 epsilon: 0.9099999999999655\n",
      "Number steps: 11500 mean reward: 1.9 epsilon: 0.8649999999999483\n",
      "Number steps: 12000 mean reward: 1.0 epsilon: 0.819999999999931\n",
      "Number steps: 12500 mean reward: 2.6 epsilon: 0.7749999999999138\n",
      "Number steps: 13000 mean reward: 3.0 epsilon: 0.7299999999998965\n",
      "Number steps: 13500 mean reward: 1.7 epsilon: 0.6849999999998793\n",
      "Number steps: 14000 mean reward: 2.8 epsilon: 0.639999999999862\n",
      "Number steps: 14500 mean reward: 1.1 epsilon: 0.5949999999998448\n",
      "Number steps: 15000 mean reward: 2.5 epsilon: 0.5499999999998275\n",
      "Number steps: 15500 mean reward: 1.7 epsilon: 0.5049999999998103\n",
      "Number steps: 16000 mean reward: 1.8 epsilon: 0.4599999999998177\n",
      "Number steps: 16500 mean reward: 2.6 epsilon: 0.41499999999982823\n",
      "Number steps: 17000 mean reward: 1.7 epsilon: 0.36999999999983874\n",
      "Number steps: 17500 mean reward: 2.3 epsilon: 0.32499999999984924\n",
      "Number steps: 18000 mean reward: 4.3 epsilon: 0.27999999999985975\n",
      "Number steps: 18500 mean reward: 4.5 epsilon: 0.23499999999986562\n",
      "Number steps: 19000 mean reward: 3.1 epsilon: 0.18999999999986225\n",
      "Number steps: 19500 mean reward: 2.6 epsilon: 0.14499999999985888\n",
      "Number steps: 20000 mean reward: 2.5 epsilon: 0.09999999999985551\n",
      "Number steps: 20500 mean reward: 2.4 epsilon: 0.09999999999985551\n",
      "Number steps: 21000 mean reward: 2.1 epsilon: 0.09999999999985551\n",
      "Number steps: 21500 mean reward: 2.1 epsilon: 0.09999999999985551\n",
      "Number steps: 22000 mean reward: 2.7 epsilon: 0.09999999999985551\n",
      "Number steps: 22500 mean reward: 2.5 epsilon: 0.09999999999985551\n",
      "Number steps: 23000 mean reward: 2.0 epsilon: 0.09999999999985551\n",
      "Number steps: 23500 mean reward: 3.2 epsilon: 0.09999999999985551\n",
      "Number steps: 24000 mean reward: 2.3 epsilon: 0.09999999999985551\n",
      "Number steps: 24500 mean reward: 2.3 epsilon: 0.09999999999985551\n",
      "Number steps: 25000 mean reward: 3.6 epsilon: 0.09999999999985551\n",
      "Successful episodes %: 2.296\n"
     ]
    }
   ],
   "source": [
    "rewards, _ = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Network Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1bb57fabe0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mean reward over time\n",
    "reward_mat = np.resize(np.array(rewards), [len(rewards) // 100, 100])\n",
    "mean_reward = np.average(reward_mat, 1)\n",
    "plt.plot(mean_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "A number of improvements above and beyond the DQN architecture described by DeepMind (http://www.davidqiu.com:8888/research/nature14236.pdf), have allowed for even greater performance and stability.\n",
    "\n",
    "### Double DQN\n",
    "\n",
    "The main intuition behind Double DQN is that the regular DQN often overestimates the Q-values of the potential actions to take in a given state. While this would be fine if all actions were always overestimates equally, there was reason to believe this wasn’t the case. You can easily imagine that if certain suboptimal actions regularly were given higher Q-values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. Below is the new DDQN equation for updating the target value.\n",
    "\n",
    "Q-Target = r + γQ(s’,argmax(Q(s’,a,ϴ),ϴ’))\n",
    "\n",
    "### Dueling DQN\n",
    "\n",
    "In order to explain the reasoning behind the architecture changes that Dueling DQN makes, we need to first explain some a few additional reinforcement learning terms. The Q-values that we have been discussing so far correspond to how good it is to take a certain action given a certain state. This can be written as Q(s,a). This action given state can actually be decomposed into two more fundamental notions of value. The first is the value function V(s), which says simple how good it is to be in any given state. The second is the advantage function A(a), which tells how much better taking a certain action would be compared to the others. We can then think of Q as being the combination of V and A. More formally:\n",
    "\n",
    "Q(s,a) =V(s) + A(a)\n",
    "\n",
    "The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. It may seem somewhat pointless to do this at first glance. Why decompose a function that we will just put back together? The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time. For example: imagine sitting outside in a park watching the sunset. It is beautiful, and highly rewarding to be sitting there. No action needs to be taken, and it doesn’t really make sense to think of the value of sitting there as being conditioned on anything beyond the environmental state you are in. We can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions.\n",
    "\n",
    "(From https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
